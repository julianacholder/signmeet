# Real-Time Sign Language Translation System
## Initial Demo - Virtual Job Interview Assistant

A real-time sign language to speech translation system designed to help deaf professionals participate in virtual job interviews using AI-powered MediaPipe hand tracking and transformer-based translation models.

---

##  Links

**GitHub Repository:** [https://github.com/julianacholder/signmeet.git]

**Demo Video:** [https://drive.google.com/drive/folders/1TJoZzxWRxUiFDOlRXzikm_QOpE8FcE9A?usp=sharing]

**Figma Design:** [https://www.figma.com/design/Jk7xeZ3VM8dB1050tw8O4C/SignMeet?node-id=0-1&t=Id7qj200lzLDKOBU-1]

---

##  Project Overview

This initial demo showcases the core functionality of a sign language translation system that bridges communication gaps for deaf job seekers in virtual interviews. The system captures hand gestures via webcam, processes them using MediaPipe, and translates them into text/speech in real-time.

### Key Features (Demo Version)
- Real-time hand tracking and gesture recognition
- Basic sign language translation (limited vocabulary)
- Practice interview mode
- User profile management
- Simple, accessible UI design

---

##  Tech Stack

**Frontend:**
- React.js
- TailwindCSS
- MediaPipe Hands API

**Backend:**
- Python/Flask or Node.js
- TensorFlow/PyTorch (for ML models)

**ML Components:**
- MediaPipe for hand landmark detection
- Custom transformer model for sign recognition

---

##  Setup Instructions

### Prerequisites
```bash
Node.js (v16+)
Python (v3.8+)
Webcam access